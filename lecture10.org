#+title: Lecture10

* What is a file system?
- files persist
    - When process exits/system crashes, we run out of
- Files are abstract objects, must map them onto physical devices
    - (Abstract) â†’ (concrete)
    - Physical devices: Hard disks, flash drives,
- File systems are hybrid data structure:
    - Keeps track of wehre things are in RAM
    - **Performance issue**: persistance vs speed of tracking things in RAM vs Disk
** Storage hierarchy
1. Registers
2. Caches
3. DRAM
4. Flash/disk
5. Network Storage (magnetic tape)

* Metrics for I/O
I/O bound applications don't care about cpu usage
** Utilization:
   - % of IO apacity (inputs/outputs per second) in use
   - problem is Utilization is very low
   - what you tell the people buying the server: higher is good (You're using 98% of the machine!)
** Throughput:
   - 1/time,
   - rate of request completion
   - what you tell the users: higher is good (we can handle 10k requests!)
** Latency:
   - time between request & response
   - Latency is usually the significant metric for perceived performance
     - Tradeoff with throughput: can't handle many at once = applications don't have to wait
* locality of reference
Locality: assumption about the general behavior of a program

** spatial locality:
- if you access a[i], likely to access a[i+1] and a[i-1]
- devices can batch data, so we want to batch things around the same location
** Temporal Locality
- if you access a[i] at time t, likely to access a[i] at time t+1 (or t+100!)

* File System Levels
- upper level fun stuff (later) / block level (lower level)
** Block Level
- divide into an array of blocks, size of the block depends on the file system
- read block i, write block i
| 8KiB | 8KiB | ........... | 8KiB |
- blocks let you take advantage of temporal and spatial locality
  - Kernel has a cache in ram (with blocks same size as file system blocks), remembers a small subset of the blocks in memory
  - Read data will go into the cache first, then be read by the program
  - Writing: copies into the kernel cache, marks the cache as "to be written to secondary storage"
    - **problem** data is not persistent yet!
    - more problems if system crashes
      - Ex: one thread writes, another thread reads, both to I/O but only read from the cache, might not be stored yet (especially if something crashes)
    - Solution: `sync()`: saves data & meta data for all open files (ensure nothing stored in cache only)
      - fsync(fd) will do this for one file descriptor (cheaper)
      - fdatasync(fd): won't save meta data, but saves the meta data (cheapest)
** Cache Coherence
  - Problems described above is cache coherance
*** Cache size
- Unused memory is wasted memory
*** scheduling:
- many competing processes want to use the cache
- For I/O bound applications, this scheduling has a large effect on performance
- Evection policy: which cache block to kick out
- Cache size is fixed at runtime
* Speculation:
- Motivation: we want to read/write from cache, not the secondary (for speed)
- Speculation OS guesses what a program wants, instead of waiting for the request
** Prefetching:
- Guessing what the computer is going to ask (read)
** Batching
- grab 10 blocks when a user asks for 1 block (read)
- Works for writes too: writes it out all at once
** Dallying
- Don't write immediately, record the requests then do them all at once at a different time
- if you dally long enough, can issue one big write
- Build a bigger batch with dallying
** Speculation: user level vs kernel code
- Most of these tricks can be used in the user, but kernel wants to make that irrelevant
- User level should not be able to know how the kernel is doing block writes
- user can inform kernel to stop speculating (madvise())
* Disk drives
- multiple magnetic disks on top of eachother
- 7200 rpm = 120Hz
** how it works
- disk head a couple atoms away from the disk, reading
- Like a vinyl player, but the needle can move
- can only read or write when the head isn't move
- Data you can access quickly is a cyclinder (chunk of it is called a sector)
- reading a random sector:
  - Send a command to the disk controller
  - Seek step: read heads move to get to the right cylinder (~10 ms) (dominant time)
  - Rotational latency: once at the right cylinder, find right sector, (~4ms) (dominant time)
  - Transfer time from disk to controller cache
  - Transfer time from controller cache to ram
* Disk Scheduling
** Goals:
- high throughput
- no starvation
- Focusing on the seek time
** Model
- Seek time only
  - No rotational latency considered
- 1 magnetic platter, therefore we represent memory this way (as if a circle)
| 0 | ... | ... | ... | ... | ... | 1 | // time for the read |
|   |     |     |     |     |     |   |                      |
            h      i
- cost: |i-h|
- Average cost for random reads:
  - worst case: read of 1, best case: read of 0
  - [ ] REVIEW: Average: integral (0,1) integral (0,1) |i-h| di dh = 1/3
** FCFS
** Shortest Seek Time First:
- minimizie |i-h| for all pending requests
- gravitate to where the requests are,
- High throughput, VERY low latency (+ starvation)
  - If requests keep coming in near the head, veteran processes far from the head will get starved
** FCFS + SSTF
- break input queue into chunks
- Within each chunk, do SSTF, but must finish a chunk of I/O requests before going to the next chunk
** Elevator algorithm
- elevator goes in increasing direction, then decreasing
  - Can only seek forward, until hits boundary and seeks backwards anyways
- Circular elevator algorithm: goes in increasing direction, never backwards
  - has a fairness advantage,
  - Less throughput, have to move disk arm back to the start
    - Note: bottom floor is inner most cylinder, top flor is outter most cylinder
** Anticipatory Scheduling
- Dally a bit after an I/O
- might get an I/O with spatial/temporal locality, thus read head will be in the right position efficiency improves
- Idea: a "last call" before the elevator leaves
* Flash Drives
** properties
- Quickly changing
- Random access is cheap/fast (no seek or rotational latency)
  - Transfer rate stays the same
- Flash wears out w/ a certain number of writes
- Must erase before writing
  - writing is a 2 step process: erasing, THEN writing
  - erasing is slower than writing
** TODO Model of a flash
- 3D array of cells
- I/O ops are parallel across rows/columns/planes (slices of the array)
- page= array of cells (4KiB) (+ error correction in case reads) (1D)
  - like a list of bits
  - Unit of I/O
  - Read/write granularity (only write on erased pages)
- erasure block = array of pages (32MiB) (bigger 1D)
  - Erase a block, then you can write to pages in that block
  - Cannot "rewrite", once you write you must erase and write to effectively write
  - erasure granularity
- plane = array of erausre blocks (2D)
- Channel (or die)= array of planes (3D, many pages stacked on top of eachother)
** Problem with flash file systems
- too low level
- flash wears out when erasing,
  - Conventional file systems want to write to a file multiple times
  - ex: writing into /temp file
** Flash translation layer (FTL)
- flash controller: Hardware layer between I/O bus and CPU
  - Flash controller in charge of FTL, talks to the planes, etc
- Abstracts the geometry & erasure blocks
  - Illusion of having N pages of data, can read or write a random page
- maintains internal mapping between physical (flash drive device layer) and logical pages (cpu level)
  - Constanly updating the mapping (i.e. changing which erasure block is temp), to evenly balance the writes
  - Ex: writing to temp
    - start off storing in block 297,
    - on next write to temp, b297 is blocked & allocates a new block
    - FTL maps temp to new block
    - writes at logical level might be random, but writes in an erasure block is sequential
      - [ ] what does sequential writes mean? how is this significant
    - some RAM used to store the block
- Goal: wear leveling: device wears out evenly
* Types of flashdrives:
- Old: SATA drives,
  - act like disk drives
- New: NVMe Drives
* NVMe Non volatime Memory
- Speed up hardware architecture of FTL, speed it up (increase throughput)
- Goal: provide interface increase throughput access to flash drives
- [X] How does this make it faster: Does Loads & stores into RAM!, not through programmed I/O (like beginning of class)
- interface has simple set of commands for I/O:
  - Read: the Ith page
  - write: the Ith page
    - R&W will be implemented at the low level
  - flush: make sure the writes have actually happened (cache writing)
    - make sure the cache's changes are committed
** How it works: I/O submit queue in dram:
- CPU has own cache & DRAM controller
- DRAM
- NVMe controller talks to DRAM (like DMA)
- flash drive talks to DRAM
*** Data structures inside DRAM
- [ ] check camera roll & rewatch lectures
- I/O submit queue: bufer of I/O requests
  - when buffer fills up, that means flash controller isn't keeping up with CPU
- I/O Submit queue: completing requets
  - Responses back from controller to the CPU, saying what it finished
- Doesn't need to be completed in order they were made
  - cpu reads what is done
  - Can still put dependencies into the queue, so not completely random

* ZNS NVMe (Zone Name System)
- add another abstriction to hardware level
** How it works
- Partition the drive into zones (zones vary in size)
  - can specify how u want the partitions
- zones are like erasure blocks, but varying size
- single zone can write to the zones as much as you want, must write to it in sequential order
  - erase them, then write to the end (aka append to them)
- Zones have states:
  - Empty, open, close, full, read only, offline
  - Look up what these states mean
** Addressing zones:
- Zone # + page #
  - coarse grained addressing
  - user level doesn't worry about garbage collection
    - no need to delete garbage blocks
** Blackboard analogy
- Send several requests in ||, executed out of order (for efficiency)
- catches of writing:
  - write (specifies page number#) //request specifies the page number
    - must follow rules of appending
    - responsibility on OS to keep track of what's been written
    - os keeps track of what is being written
    - At most one pending write requests
      - output queue cannot be filled with output requests
    - Request specifies the page number
  - Append to a zone(whatever ) //response specifies the page number
    - advantage: can have lots of pending requests
    - find out where they went after they've gone
    - Response specifies the page number
      - OS must deal with the fact that the page number is essentially random
- Speed increase comes from the Append
